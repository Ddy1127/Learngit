{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut tree centroid detection using U-Net\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing libraries'''\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Conv1D, Concatenate, Conv2DTranspose, GaussianNoise, Lambda, Dropout, Flatten, Dense, Reshape, TimeDistributed, Permute, Softmax, Multiply, BatchNormalization, UpSampling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import glob\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接受一个参数作为高斯分布的范围，然后生成一个以均值为0，标准差为给定范围一半的二维高斯分布数组\n",
    "def __getGaussian(radius):\n",
    "    x = np.linspace(-radius, +radius, radius*2)\n",
    "    y = np.linspace(-radius, +radius, radius*2)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    d = np.sqrt(xx**2+yy**2)\n",
    "    sigma, mu = radius/2, 0.0\n",
    "    gauss = np.exp(-( (d-mu)**2 / ( 2.0 * sigma**2 ) ) )\n",
    "    gauss = ( gauss - np.min(gauss) ) / ( np.max(gauss) - np.min(gauss) ) # scalling between 0 to 1\n",
    "\n",
    "    return gauss\n",
    "\n",
    "#将给定的点列表表示的位置信息转换为图像表示，其中每个点被表示为一个高斯分布的区域\n",
    "def centroids2Images(point_list, im_num_row, im_num_col, g_radius=20):\n",
    "\n",
    "    circle_mat = __getGaussian(g_radius)\n",
    "\n",
    "    temp_im = np.zeros((im_num_row+g_radius*2, im_num_col+g_radius*2))\n",
    "\n",
    "    for one_pnt in point_list:\n",
    "        pnt_row = int(one_pnt[0])\n",
    "        pnt_col = int(one_pnt[1])\n",
    "\n",
    "        current_patch = temp_im[g_radius+pnt_row-g_radius:g_radius+pnt_row+g_radius, g_radius+pnt_col-g_radius:g_radius+pnt_col+g_radius]\n",
    "        temp_im[g_radius+pnt_row-g_radius:g_radius+pnt_row+g_radius, g_radius+pnt_col-g_radius:g_radius+pnt_col+g_radius] = np.maximum(current_patch, circle_mat)\n",
    "\n",
    "    temp_im = temp_im[g_radius:-g_radius, g_radius:-g_radius]\n",
    "\n",
    "    return temp_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reading training data'''\n",
    "in_h = 1280 # model input height\n",
    "in_w = 1280 # model input width\n",
    "imList = glob.glob(\"./data/TongoCoconutTree/train/*.tiff\")\n",
    "\n",
    "x_train = np.zeros((len(imList), in_h, in_w, 3)).astype('float32')\n",
    "y_train = np.zeros((len(imList), in_h, in_w, 1)).astype('float32')\n",
    "\n",
    "for i1 in range(len(imList)):\n",
    "    x_train[i1,:,:,:] = imageio.imread(imList[i1])\n",
    "    \n",
    "    with open(imList[i1].replace('train', 'train_labels').replace('tiff', 'json')) as f:\n",
    "        ctr_list = json.load(f)['centroids']\n",
    "        ctr_img = centroids2Images(ctr_list, im_num_row=in_h, im_num_col=in_w, g_radius=40)\n",
    "        \n",
    "        y_train[i1,:,:,0] = ctr_img\n",
    "        \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''visualizing random input/output training images'''\n",
    "\n",
    "rdm_idx = randint(0, len(imList))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(x_train[0,:,:,:].astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_train[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reading val data'''\n",
    "\n",
    "imList = glob.glob(\"./data/TongoCoconutTree/val/*.tiff\")\n",
    "\n",
    "x_val = np.zeros((len(imList), in_h, in_w, 3)).astype('float32')\n",
    "y_val = np.zeros((len(imList), in_h, in_w, 1)).astype('float32')\n",
    "\n",
    "for i1 in range(len(imList)):\n",
    "    x_val[i1,:,:,:] = imageio.imread(imList[i1])\n",
    "    \n",
    "    with open(imList[i1].replace('val', 'val_labels').replace('tiff', 'json')) as f:\n",
    "        ctr_list = json.load(f)['centroids']\n",
    "        ctr_img = centroids2Images(ctr_list, im_num_row=in_h, im_num_col=in_w, g_radius=40)\n",
    "        y_val[i1,:,:,0] = ctr_img\n",
    "        \n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''visualizing random input/output val images'''\n",
    "\n",
    "rdm_idx = randint(0, len(imList))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(x_val[0,:,:,:].astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_val[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Model Development and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(input_shape):\n",
    "\n",
    "    e_in = Input(shape = input_shape)\n",
    "\n",
    "    '''Encoder'''\n",
    "\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_in)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_skip1 = Dropout(0.2)(e_cnn)\n",
    "    e_cnn = MaxPooling2D(pool_size=(2, 2))(e_skip1)\n",
    "\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_skip2 = Dropout(0.2)(e_cnn)\n",
    "    e_cnn = MaxPooling2D(pool_size=(2, 2))(e_skip2)\n",
    "\n",
    "    e_cnn = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_skip3 = Dropout(0.2)(e_cnn)\n",
    "    e_cnn = MaxPooling2D(pool_size=(2, 2))(e_skip3)\n",
    "\n",
    "    e_cnn = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_skip4 = Dropout(0.2)(e_cnn)\n",
    "    e_cnn = MaxPooling2D(pool_size=(2, 2))(e_skip4)\n",
    "\n",
    "    '''Middle Part'''\n",
    "\n",
    "    e_cnn = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Dropout(0.2)(e_cnn)\n",
    "    e_cnn = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "\n",
    "    '''Decoder'''\n",
    "\n",
    "    e_cnn = UpSampling2D(size = (2,2))(e_cnn)\n",
    "    e_cnn = Concatenate()([e_cnn,e_skip4])\n",
    "    e_cnn = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Dropout(0.2)(e_cnn)\n",
    "\n",
    "    e_cnn = UpSampling2D(size = (2,2))(e_cnn)\n",
    "    e_cnn = Concatenate()([e_cnn,e_skip3])\n",
    "    e_cnn = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Dropout(0.2)(e_cnn)\n",
    "\n",
    "    e_cnn = UpSampling2D(size = (2,2))(e_cnn)\n",
    "    e_cnn = Concatenate()([e_cnn,e_skip2])\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Dropout(0.2)(e_cnn)\n",
    "\n",
    "    e_cnn = UpSampling2D(size = (2,2))(e_cnn)\n",
    "    e_cnn = Concatenate()([e_cnn,e_skip1])\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Dropout(0.2)(e_cnn)\n",
    "\n",
    "    '''Last Part'''\n",
    "\n",
    "    e_cnn = Conv2D(32, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "    e_cnn = Conv2D(32, 1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(e_cnn)\n",
    "    e_cnn = BatchNormalization()(e_cnn)\n",
    "\n",
    "    e_out = Conv2D(1, 1, activation = 'sigmoid')(e_cnn)\n",
    "\n",
    "    model = Model(inputs = e_in, outputs = e_out)\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''building U-Net model'''\n",
    "\n",
    "model = getModel(input_shape=(in_h,in_w,3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''fitting the model'''\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "fit_h = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plotting loss curves'''\n",
    "\n",
    "plt.plot(fit_h.history['loss'])\n",
    "plt.plot(fit_h.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''saving the model'''\n",
    "\n",
    "model.save_weights(\"./models/model.h5\")\n",
    "\n",
    "np.savetxt('./models/TongoCoconutTree_loss.txt', fit_h.history['loss'], fmt='%.8f')\n",
    "np.savetxt('./models/TongoCoconutTree_loss_val.txt', fit_h.history['val_loss'], fmt='%.8f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''reading test data'''\n",
    "\n",
    "imList = glob.glob(\"./data/TongoCoconutTree/test/*.tiff\")\n",
    "\n",
    "x_test = np.zeros((len(imList), in_h, in_w, 3)).astype('float32')\n",
    "y_test = np.zeros((len(imList), in_h, in_w, 1)).astype('float32')\n",
    "\n",
    "for i1 in range(len(imList)):\n",
    "    x_test[i1,:,:,:] = imageio.imread(imList[i1])\n",
    "    \n",
    "    with open(imList[i1].replace('test', 'test_labels').replace('tiff', 'json')) as f:\n",
    "        ctr_list = json.load(f)['centroids']\n",
    "        ctr_img = centroids2Images(ctr_list, im_num_row=in_h, im_num_col=in_w, g_radius=40)\n",
    "        y_test[i1,:,:,0] = ctr_img\n",
    "        \n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading model'''\n",
    "#select one\n",
    "model.load_weights(\"./models/model.h5\")#load self-trained model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or load a model provided by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loading model'''\n",
    "#or load provided model by\n",
    "# model.load_weights(\"./models/TongoCoconutTree.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inference'''\n",
    "\n",
    "p_test = model.predict(x_test, batch_size = 1)\n",
    "print(p_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''visualizing random input/output/prediction images'''\n",
    "\n",
    "rdm_idx = randint(0, len(imList))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(x_test[rdm_idx,:,:,:].astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(y_test[rdm_idx,:,:,0])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(p_test[rdm_idx,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave( 'sample3_rgb.png', x_test[rdm_idx,:,:,:].astype('uint8') )\n",
    "plt.imsave( 'sample3_gr.png', y_test[rdm_idx,:,:,0] )\n",
    "plt.imsave( 'sample3_pred.png', p_test[rdm_idx,:,:,0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Segmentation based on Deeplabv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this notebook is a collection of water bodies images captured by the Sentinel-2 Satellite. Each image comes with a black and white mask where white represents water. The masks were created to detect and measure vegetation in satellite images.\n",
    "\n",
    "In this notebook, I applied the pretrained DeepLabV3 ResNet-50 model in Pytorch to perform segmentation on the water body images. I chose the DeepLabV3 semantic segmentation architecture because of its effectiveness and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Prepare Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#!pip install imagehash\n",
    "import imagehash\n",
    "\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "# Check if GPU parallel computing is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Prepare dataset\n",
    "# Here I selected large images height and width to improve training performance\n",
    "import glob\n",
    "height = width = 500 \n",
    "batch = 11\n",
    "\n",
    "images_list = sorted(glob.glob(\"./data\\Water Bodies Dataset\\Water Bodies Dataset\\Images/*.jpg\"))\n",
    "masks_list = sorted(glob.glob(\"./data\\Water Bodies Dataset\\Water Bodies Dataset\\Masks/*.jpg\"))\n",
    "\n",
    "print(len(images_list), len(masks_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, there are 2,841 photos in each Images and Masks folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Image visualization\n",
    "# plot first few images in Images and Masks folder \n",
    "f, axr = plt.subplots(8, 2, figsize=(12, 36)) \n",
    "for i in range(8):\n",
    "    idx = np.random.randint(0, len(images_list))\n",
    "    original = cv2.imread(images_list[idx])\n",
    "    mask = cv2.imread(masks_list[idx])\n",
    "    axr[i,0].imshow(original)\n",
    "    axr[i,1].imshow(mask, cmap = 'gray')\n",
    "    i +=1\n",
    "    axr[0, 0].set_title(\"IMAGE\")\n",
    "    axr[0, 1].set_title(\"MASK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see images and masks come with different shapes and even blank masks. These factors may present challenges during the transformation and training processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the statistical figures of the dataset to understand how the dimensions are distributed among the images. This will help us identify any abnormal size images that can be filtered out to improve training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics of the image dimensions\n",
    "dimen_img_list = []\n",
    "\n",
    "for img in images_list:\n",
    "    img = cv2.imread(img, cv2.COLOR_BGR2RGB)\n",
    "    dimen_img = img.shape[:2]\n",
    "    dimen_img_list.append(dimen_img)\n",
    "\n",
    "# Convert the list to numpy array\n",
    "dimen_img_array = np.array(dimen_img_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics of the image dimensions\n",
    "print(\"Statistics of image dimensions:\")\n",
    "print(\"Minimum width:\", np.min(dimen_img_array[:, 1]))\n",
    "print(\"Maximum width:\", np.max(dimen_img_array[:, 1]))\n",
    "print(\"Mean width:\", np.mean(dimen_img_array[:, 1]))\n",
    "print(\"Median width:\", np.median(dimen_img_array[:, 1]))\n",
    "print(\"Standard deviation of width:\", np.std(dimen_img_array[:, 1]))\n",
    "print(\"Minimum height:\", np.min(dimen_img_array[:, 0]))\n",
    "print(\"Maximum height:\", np.max(dimen_img_array[:, 0]))\n",
    "print(\"Mean height:\", np.mean(dimen_img_array[:, 0]))\n",
    "print(\"Median height:\", np.median(dimen_img_array[:, 0]))\n",
    "print(\"Standard deviation of height:\", np.std(dimen_img_array[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(dimen_img_array[:, 1], bins=50, alpha=0.5, color='blue', label='width')\n",
    "plt.hist(dimen_img_array[:, 0], bins=50, alpha=0.5, color='red', label='height')\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Image Dimensions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical result suggests that the median values of height and width of the images are around 300 pixels and therefore, it would be advisable to resize all images and masks to this size or larger to ensure good training performance. To filter out any images with abnormal sizes, a cut-off threshold of 32 pixels (which is 10% of the median value) will be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Data Cleaning\n",
    "\n",
    "#Detect duplicate images and masks\n",
    "hashes = {}\n",
    "to_remove = []\n",
    "\n",
    "for file in images_list:\n",
    "    if file.endswith('.jpg'):\n",
    "        with open(file, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            # Compute the hash value for the image\n",
    "            h = imagehash.phash(img)\n",
    "            \n",
    "            # Check if the hash value already exists in the dictionary\n",
    "            if h in hashes:\n",
    "                print(f'Duplicate image found: {file} and {hashes[h]}')\n",
    "                to_remove.append(file)\n",
    "                mask_file = os.path.join(\"./data/Water Bodies Dataset/Water Bodies Dataset/Masks\", os.path.basename(file))\n",
    "                to_remove.append(mask_file)\n",
    "            else:\n",
    "                hashes[h] = file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing 17 duplicate images and their corresponding masks, new lists of images and masks were created for the next stage of data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list of filenames that excludes the duplicates\n",
    "new_images_list = [file for file in images_list if file not in to_remove]\n",
    "new_masks_list = [os.path.join(\"./data/Water Bodies Dataset/Water Bodies Dataset/Masks\", os.path.basename(file)) for file in new_images_list]\n",
    "print(len(new_images_list), len(new_masks_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "min_size = 32\n",
    "df_images = []\n",
    "df_masks = []\n",
    "\n",
    "for img, mask in zip(new_images_list, new_masks_list):\n",
    "    # Load images in RGB and grayscale\n",
    "    n = cv2.imread(img, cv2.COLOR_BGR2RGB)\n",
    "    m = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Check if the image and mask exist and meet the size and content criteria\n",
    "    if n is not None and m is not None and min(n.shape[:2]) > min_size and ((m != 0).any() and (m != 255).any()):\n",
    "        # Resize images and masks to a consistent shape (e.g., 256x256)\n",
    "        n = cv2.resize(n, (256, 256))  # Use desired dimensions\n",
    "        m = cv2.resize(m, (256, 256))  # Use the same dimensions\n",
    "\n",
    "        df_images.append(n)\n",
    "        df_masks.append(m)\n",
    "\n",
    "# Convert lists of images and masks to NumPy arrays\n",
    "df_images = np.array(df_images)\n",
    "df_masks = np.array(df_masks)\n",
    "\n",
    "print(len(df_images), len(df_masks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 126 images and their corresponding masks were removed from the dataset, leaving a total of 2698 samples for training, testing, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessing pipeline for images and masks\n",
    "tfImg = tf.Compose([\n",
    "        tf.ToPILImage(),\n",
    "        tf.Resize((height, width)),\n",
    "        tf.ToTensor(),\n",
    "        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "tfMsk = tf.Compose([\n",
    "        tf.ToPILImage(),\n",
    "        tf.Resize((height, width)),\n",
    "        tf.ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read images and masks randomly\n",
    "def ReadImage(df_images, df_masks):\n",
    "    idx = np.random.randint(0, len(df_images))\n",
    "    Img = tfImg(df_images[idx])\n",
    "    Msk = tfMsk(df_masks[idx])\n",
    "    return Img, Msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load images and masks in batch\n",
    "def LoadBatch(df_images, df_masks):\n",
    "    images = torch.zeros([batch, 3, height, width])\n",
    "    masks = torch.zeros([batch, height, width])\n",
    "    for i in range(batch):\n",
    "        images[i], masks[i] = ReadImage(df_images, df_masks)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Split data into train, test and validation sets\n",
    "\n",
    "X = df_images\n",
    "y = df_masks\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.015, random_state=1)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluate Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Define pretrained ResNet-50 model using TorchVision DeepLabV3 architecture\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained = True)\n",
    "\n",
    "# Since the segmented images have 2 classes (black and white), we change the final layer to 2 classes\n",
    "model.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size = (1, 1), stride = (1, 1)) \n",
    "model = model.to(device)\n",
    "\n",
    "# Select Stochastic gradient descent (SGD) optimizer with a learning rate of 0.01 (optimum for image segmentation)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "Losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Evaluation of train dataset\n",
    "epochs = 10\n",
    "best_loss = float('inf')  # Initialize best_loss to a very high value\n",
    "\n",
    "# Training loop\n",
    "for itr in range(epochs):\n",
    "   images, masks = LoadBatch(X_train, y_train)\n",
    "   # Load images and masks tensors to the GPU \n",
    "   images = torch.autograd.Variable(images,requires_grad = False).to(device) \n",
    "   masks = torch.autograd.Variable(masks, requires_grad = False).to(device) \n",
    "   \n",
    "   # Forward training loop\n",
    "   Pred = model(images)['out']\n",
    "   model.zero_grad()\n",
    "   Loss = criterion(Pred, masks.long())\n",
    "   Losses.append(Loss.item())\n",
    "\n",
    "   # Backward training loop\n",
    "   Loss.backward() \n",
    "   optimizer.step() # Apply gradient descent to optimize weights\n",
    "\n",
    "   # Check if the current loss is lower than the best recorded loss\n",
    "   current_loss = Loss.item()\n",
    "   if current_loss < best_loss:\n",
    "      best_loss = current_loss\n",
    "      # Save the model\n",
    "      torch.save(model.state_dict(), 'best_model.pth')\n",
    "      print(f\"Saved better model with loss {current_loss}\")\n",
    "\n",
    "   seg = torch.argmax(Pred[0], 0).cpu().detach().numpy() # Move the tensor to the CPU, detach it from the graph, and convert it to a NumPy array\n",
    "   print(itr,\") Loss=\", Loss.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss values\n",
    "plt.plot(Losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training the model show that the loss values were slightly better than the TensorFlow model (0.12 vs. 0.34). It seems that the optimal number of epochs for achieving good loss values is around 1000, and increasing the number of epochs beyond this point does not significantly improve training performance. \n",
    "\n",
    "Now that the model has been trained, we can proceed to testing it with new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Finalize Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained model weights by yourself\n",
    "model.load_state_dict(torch.load(r'D:\\gxy\\01_lesson\\00use_Deep-Learning-Projects-main\\Deep-Learning-Projects-main\\best_model.pth'))\n",
    "# Load the pretrained optimal model weights\n",
    "#model.load_state_dict(torch.load(r'./models/water/water_best_model_epoch1000.pth'))\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set trained model to the evaluation mode for validation test set\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses_test = []\n",
    "\n",
    "with torch.no_grad(): # Set context to not calculate and save gradients\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for itr in range(epochs):\n",
    "   images_test, masks_test = LoadBatch(X_test, y_test)\n",
    "   # Load images and masks tensors to the GPU \n",
    "   images_test = torch.autograd.Variable(images_test,requires_grad = False).to(device) \n",
    "   masks_test = torch.autograd.Variable(masks_test, requires_grad = False).to(device) \n",
    "   \n",
    "   Pred_test = model(images_test)['out']\n",
    "   model.zero_grad()\n",
    "   Loss_test = criterion(Pred_test, masks_test.long())\n",
    "   Losses_test.append(Loss_test.item())\n",
    "\n",
    "   seg = torch.argmax(Pred_test[0], 0).cpu().detach().numpy() # Move the tensor to the CPU, detach it from the graph, and convert it to a NumPy array\n",
    "   print(itr,\") Loss=\", Loss_test.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss values\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(Losses, label = 'training')\n",
    "plt.plot(Losses_test, label = 'testing')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consistency between the training and validation losses suggests that the model is capable of accurately predicting on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Predictions on validation data\n",
    "Pred_val = []\n",
    "with torch.no_grad(): # Set context to not calculate and save gradients\n",
    "    for i in X_val:\n",
    "        Img_tensor = tfImg(i)\n",
    "        # Load tensor images to GPU    \n",
    "        Img_tensor = torch.autograd.Variable(Img_tensor, requires_grad = False).to(device).unsqueeze(0)\n",
    "        Pred2 = model(Img_tensor)['out']\n",
    "        Pred_val.append(Pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np_list = []\n",
    "for i in range(len(Pred_val)):\n",
    "    #Get original shape of images in X_val\n",
    "    height_orgin, width_orgin, d = X_val[i].shape \n",
    "    pred_tensor = Pred_val[i]\n",
    "    # Resize segmented images to origninal sizes\n",
    "    pred_resized = tf.Resize((height_orgin,width_orgin))(pred_tensor[0]) \n",
    "    #Convert the tensor image to a numpy array\n",
    "    pred_np = pred_resized.cpu().detach().numpy()\n",
    "    pred_np_list.append(pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and predicted images\n",
    "fig, ax = plt.subplots(len(X_val), 3, figsize=(12,40))\n",
    "for i in range(len(X_val)):\n",
    "    orig = X_val[i]\n",
    "    msk = y_val[i]\n",
    "    seg = pred_np_list[i][1, :, :]\n",
    "    ax[i,0].imshow(orig)\n",
    "    ax[i,1].imshow(msk, cmap='gray')\n",
    "    ax[i,2].imshow(seg, cmap='gray')\n",
    "    i +=1\n",
    "    ax[0, 0].set_title(\"IMAGE\")\n",
    "    ax[0, 1].set_title(\"MASK\")\n",
    "    ax[0, 2].set_title(\"PREDICTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data cleaning is essential to remove duplicate, blank and abnormal size images.\n",
    "\n",
    "- Increasing the size of the images, batch size, the number of epochs and size of dataset can enhance the performance of the training process.\n",
    "\n",
    "- The DeepLabV3 architecture is a simple and highly effective pretrained model for semantic segmentation in pytorch.\n",
    "\n",
    "- When comparing the validation loss of the pretrained Resnet-50 model in Pytorch to that of the Keras model (0.12 vs. 0.34), we can see the computational demands of the DeepLabV3 model are significantly higher but its prediction accuracy is about the same as the Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Segmentation based on Deeplabv3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
    "import segmentation_models_pytorch as smp\n",
    "print(dir(smp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & Create train / valid splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/Road Dataset/'\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n",
    "metadata_df = metadata_df[metadata_df['split']=='train']\n",
    "metadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\n",
    "metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "# Shuffle DataFrame\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Perform 90/10 split for train / val\n",
    "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
    "train_df = metadata_df.drop(valid_df.index)\n",
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n",
    "# Get class names\n",
    "class_names = class_dict['name'].tolist()\n",
    "# Get class RGB values\n",
    "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortlist specific classes to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = ['background', 'road']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for viz. & one-hot encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"DeepGlobe Road Extraction Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        df (str): DataFrame containing images / labels paths\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            df,\n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.image_paths = df['sat_image_path'].tolist()\n",
    "        self.mask_paths = df['mask_path'].tolist()\n",
    "        \n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Sample Image and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RoadsDataset(train_df, class_rgb_values=select_class_rgb_values)\n",
    "random_idx = random.randint(0, len(dataset)-1)\n",
    "image, mask = dataset[2]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        album.HorizontalFlip(p=0.5),\n",
    "        album.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"\n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Augmented Images & Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = RoadsDataset(\n",
    "    train_df, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
    "\n",
    "# Different augmentations on image/mask pairs\n",
    "for idx in range(3):\n",
    "    image, mask = augmented_dataset[idx]\n",
    "    visualize(\n",
    "        original_image = image,\n",
    "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = select_classes\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Train / Val DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = RoadsDataset(\n",
    "    train_df, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = RoadsDataset(\n",
    "    valid_df, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=3, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import utils\n",
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.00008),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, './train_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model checkpoint from the current run\n",
    "#此处加载上面刚刚训练的模型，无需修改路径，运行该代码，则无需运行下一个cell\n",
    "if os.path.exists('./train_model.pth'):\n",
    "    model = torch.load('./train_model.pth', map_location=torch.device(\"cpu\"))\n",
    "    print('Loaded DeepLabV3+ model from current train.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from previous commit (if present)\n",
    "# 如果使用提供之前训练好的最优模型,运行该代码\n",
    "# 由于直接加载最优模型，没有训练过程将无法显示损失曲线，直接运行Prediction on Test Data部分\n",
    "if os.path.exists('./models/road/road_best_model_epoch100.pth'):\n",
    "    best_model = torch.load('./models/road/road_best_model_epoch100.pth', map_location=DEVICE)\n",
    "    print('Loaded pre-trained DeepLabV3+ model!')\n",
    "# 将模型设置为评估模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader to be used with DeepLabV3+ model (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = RoadsDataset(\n",
    "    valid_df, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
    "test_dataset_vis = RoadsDataset(\n",
    "    valid_df,\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# get a random test image/mask index\n",
    "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
    "image, mask = test_dataset_vis[random_idx]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_logs = test_epoch.run(test_dataloader)\n",
    "print(\"Evaluation on Test Data: \")\n",
    "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
    "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Dice Loss & IoU Metric for Train vs. Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs_df = pd.DataFrame(train_logs_list)\n",
    "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
    "train_logs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('IoU Score', fontsize=20)\n",
    "plt.title('IoU Score Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('iou_score_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Dice Loss', fontsize=20)\n",
    "plt.title('Dice Loss Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('dice_loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds_folder = 'sample_predictions/'\n",
    "if not os.path.exists(sample_preds_folder):\n",
    "    os.makedirs(sample_preds_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):\n",
    "\n",
    "    image, gt_mask = test_dataset[idx]\n",
    "    image_vis = test_dataset_vis[idx][0].astype('uint8')\n",
    "    x_tensor = torch.from_numpy(image).to(torch.device(\"cpu\")).unsqueeze(0)\n",
    "    # 使用模型实例进行预测\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        pred_mask = best_model(x_tensor)#使用之前训练好的最优模型\n",
    "        #pred_mask = model(x_tensor)#使用现在训练的模型\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    #pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to foreground\n",
    "    pred_road_heatmap = pred_mask[:,:,select_classes.index('road')]\n",
    "    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
    "    \n",
    "    visualize(\n",
    "        original_image = image_vis,\n",
    "        ground_truth_mask = gt_mask,\n",
    "        predicted_mask = pred_mask,\n",
    "        pred_road_heatmap = pred_road_heatmap\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2554f0b3e5a40628a2f1035e6dc5cfbd612402ad9e1f31e27b21869ba932633d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
